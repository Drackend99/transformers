# Transformers
Transformers are the powerhouse behind much of state of the art text generation. How do transformers work? Let's find out.

# Sequence-to-sequence models

A sequence-to-sequence model is a model that, rather than a singular input value, takes in a sequence of values as its input value.

Under the hood, sequence to sequence models are made up of an encoder and a decoder

# Attention

The first concept we need to explain is attention. 
